{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author: Kyra Menai Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I will give a further commentary on each part of the code I have written in analysis.py. Please refer to the analysis.py python file for the analysis code in a more cohesive piece. In this file it will be broken down and annotated as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Importing the modules and specific tools for the data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any analysis can be started, importing the correct tools is essential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the importing of module libraries the dataset needed to be sourced. This was sourced from the UC Irvine Machine Learning Repository and imported using the python import button. firt it was necessary to run the 'pip install ucimlrepo' in the terminal to install the ucimlrepo package. Following this the dataset was imported. It is important to know that the dataset was initially not imported as a DataFrame, rather as a file containing metadata and variables. the dataset was renamed 'iris' to make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset, fetch the dataset, define the data (as pandas dataframes), print metadata, and print the variable information to check that it worked.\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "iris = fetch_ucirepo(id=53) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with the analysis, saving the dataset as a .csv file for future reference was important. The dataset needed to be converted and for this to happen. First the x and y frames of the data were extracted, these were the features and targets, respectively. Then the metadata and variables were checked and changed to note form. Finally the features and targets were combined into a dataframe 'iris_df' and this was converted to a .csv using the '.to_csv' function. Upon successful completion \"Iris dataset has been successfully exported to a CSV!\" would be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - extracting x and y (as pandas dataframes) \n",
    "x = iris.data.features \n",
    "y = iris.data.targets \n",
    "\n",
    "# metadata - print was to check\n",
    "# print(iris.metadata) \n",
    "\n",
    "# variable information - print was to check\n",
    "# print(iris.variables) \n",
    "\n",
    "# Combine the features and targets into a single DataFrame (df) so it can be exported as a CSV\n",
    "iris_df = pd.concat([x, y], axis=1)\n",
    "\n",
    "# Exporting the DataFrame (df) to a CSV file\n",
    "iris_df.to_csv('D:/Data_Analytics/Modules/PandS/pands-project/iris.csv', index=False)\n",
    "print(\"Iris dataset has been successfully exported to a CSV!\") # Output - Iris dataset has been successfully exported to a CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to continuing with the data analysis, to ensure ease of data manipulation, the data for analysis was then inputted from the iris dataframe saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('D:/Data_Analytics/Modules/PandS/pands-project/iris.csv')\n",
    "\n",
    "print(iris_df) # This will print the dataframe into the terminal and also gi ve a brief summary of (150 rows x 5 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to directly save any text or plots directly to a text file. This is an example with fully adapted text to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing output directly to a txt file: https://labex.io/tutorials/python-how-to-redirect-the-print-function-to-a-file-in-python-398057\n",
    "\n",
    "# FOR SAVING AS A TXT FILE AND APPENDING AS WE GO ON \n",
    "## First, create a file with some initial content\n",
    "#with open(\"append_example.txt\", \"w\") as file:\n",
    "#    print(\"\\nThis content is being added to the file.\", file=file)\n",
    "#    print(\"Appended on: X DATE\", file=file)\n",
    "    ## Now, append to the file\n",
    "#with open(\"append_example.txt\", \"a\") as file:\n",
    "#    print(\"\\nThis content is being appended to the file.\", file=file)\n",
    "#    print(\"Appended on: X DATE\", file=file)\n",
    "#print(\"Additional content has been appended to append_example.txt\")\n",
    "## Check the final content\n",
    "#print(\"\\nFinal content of the file:\")\n",
    "#with open(\"append_example.txt\", \"r\") as file:    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, basic data checks were conducted and written to a text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data checks - check for missing values, duplicates, and data types\n",
    "## Using the 'with' statement to handle file operations\n",
    "\n",
    "with open(\"basic_data_explore.txt\", \"w\") as file: # The (file=file) argument is important to remember as it makes sure Python knows to write to the file and not the terminal.\n",
    "    print(\"Basic data checks:\", file=file)\n",
    "    print(\"The shape of the dataset:\", file=file)\n",
    "    print(iris_df.shape, file=file)\n",
    "    print(\"The first 5 rows of the dataset:\", file=file)\n",
    "    print(iris_df.head(), file=file) # This will print the first 5 rows of the dataset.\n",
    "    print(\"The last 5 rows of the dataset:\", file=file)\n",
    "    print(iris_df.tail(), file=file) # This will print the last 5 rows of the dataset.\n",
    "    print(\"The column names of the dataset:\", file=file)\n",
    "    print(iris_df.columns, file=file) # This will print the column names of the dataset.\n",
    "    \n",
    "print(\"Basic data checks have been written to basic_data_explore.txt\")\n",
    "\n",
    "with open(\"basic_data_explore.txt\", \"a\") as file:\n",
    "    print(\"The number of rows and columns in the dataset:\", file=file)\n",
    "    print(iris_df.info(), file=file) # This will print the number of rows and columns in the dataset.\n",
    "    print(\"The number of missing values in the dataset:\", file=file)\n",
    "    print(iris_df.isnull().sum(), file=file) # This will print the number of missing values in the dataset.\n",
    "    print(\"The number of duplicate rows in the dataset:\", file=file)\n",
    "    print(iris_df.duplicated().sum(), file=file) # This will print the number of duplicate rows in the dataset.\n",
    "    print(\"The data types of each column in the dataset:\", file=file)\n",
    "    print(iris_df.dtypes, file=file) # This will print the data types of each column in the dataset.print(\"This is the initial content of the file.\", file=file)\n",
    "    \n",
    "print(\"Basic data checks have been appended to basic_data_explore.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting additional data analysis, it is important to remove any duplicate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make sure tha any duplicates are removed and that the data types are correct before conducting any analysis.\n",
    "# Already checked for missing values and we know there are 0, but there are 3 duplicate rows in the dataset.\n",
    "\n",
    "data = iris_df.drop_duplicates(subset=\"class\",) # This will remove any duplicate rows in the dataset, based on the class(species) column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Statistics for each of the variables is conducted and written to a text file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise each variable in the dataset and check for outliers - export to a single text file.\n",
    "\n",
    "with open(\"summary_statistics.txt\", \"w\") as file:  # New file for summary stats\n",
    "    # Summary statistics for each species\n",
    "    print(\"Summary statistics for each species:\", file=file)\n",
    "    \n",
    "    # Separate the dataset by species\n",
    "    setosa_stats = iris_df[iris_df['class'] == 'Iris-setosa'].describe()\n",
    "    versicolor_stats = iris_df[iris_df['class'] == 'Iris-versicolor'].describe()\n",
    "    virginica_stats = iris_df[iris_df['class'] == 'Iris-virginica'].describe()\n",
    "\n",
    "    # Display the statistics for each species\n",
    "    print(\"Setosa Statistics:\", file=file)\n",
    "    print(setosa_stats, file=file)\n",
    "\n",
    "    print(\"\\nVersicolor Statistics:\", file=file)\n",
    "    print(versicolor_stats, file=file)\n",
    "\n",
    "    print(\"\\nVirginica Statistics:\", file=file)\n",
    "    print(virginica_stats, file=file)\n",
    "\n",
    "print(\"Summary statistics for each species has been written to summary_statistics.txt\")\n",
    "\n",
    "with open(\"summary_statistics.txt\", \"a\") as file:  # Append to summary stats file\n",
    "    # Checking for outliers in the dataset\n",
    "\n",
    "    # Function to detect outliers using the inter-quartile range method\n",
    "    def detect_outliers(df, column):\n",
    "        Q1 = df[column].quantile(0.25)  # First quartile\n",
    "        Q3 = df[column].quantile(0.75)  # Third quartile\n",
    "        IQR = Q3 - Q1  # Interquartile range\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        return outliers\n",
    "    # Check for outliers in each numeric column for each species\n",
    "    numeric_columns = iris_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    print(\"\\nOutliers detected for each species:\", file=file)\n",
    "    for species in ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']:\n",
    "        print(f\"\\nOutliers for {species}:\", file=file)\n",
    "        species_data = iris_df[iris_df['class'] == species]\n",
    "        for column in numeric_columns:\n",
    "            outliers = detect_outliers(species_data, column)\n",
    "            if not outliers.empty:\n",
    "                print(f\"  Column '{column}': {len(outliers)} outliers\", file=file)\n",
    "            else:\n",
    "                print(f\"  Column '{column}': No outliers detected\", file=file)\n",
    "\n",
    "print(\"Checks for data outliers has been appended to summary_statistics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots to evaluate dataset and visualise any outliers within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots - plot and save box plots for each variable in the dataset and save as a png file.\n",
    "\n",
    "# Boxplots by species\n",
    "\n",
    "# Define feature names and their corresponding titles\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "titles = ['Sepal Length by Species', 'Sepal Width by Species', \n",
    "          'Petal Length by Species', 'Petal Width by Species']\n",
    "\n",
    "# Create boxplots for each feature by species\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(features):\n",
    "    ax = plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x='class', y=feature, hue='class', data=iris_df, ax=ax)\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel(\"Species\")  # Update x-axis label for clarity\n",
    "    ax.set_ylabel(feature.replace('_', ' ').title())  # Format y-axis label\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('boxplots_by_species.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms looking at frequency vs. feature measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms - plot and save histograms for each variable in the dataset as a png file.\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot histogram for Sepal Length\n",
    "sns.histplot(data=iris_df, x=\"sepal length\", hue=\"class\", kde=False, ax=axes[0, 0], bins=15)\n",
    "axes[0, 0].set_title(\"Sepal Length Distribution by Species\")\n",
    "axes[0, 0].set_xlabel(\"Sepal Length (cm)\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "axes[0, 0].legend(title=\"Species\", labels=iris_df['class'].unique(), loc='upper right')\n",
    "\n",
    "# Plot histogram for Sepal Width\n",
    "sns.histplot(data=iris_df, x=\"sepal width\", hue=\"class\", kde=False, ax=axes[0, 1], bins=15)\n",
    "axes[0, 1].set_title(\"Sepal Width Distribution by Species\")\n",
    "axes[0, 1].set_xlabel(\"Sepal Width (cm)\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "axes[0, 1].legend(title=\"Species\", labels=iris_df['class'].unique(), loc='upper right')\n",
    "\n",
    "# Plot histogram for Petal Length\n",
    "sns.histplot(data=iris_df, x=\"petal length\", hue=\"class\", kde=False, ax=axes[1, 0], bins=15)\n",
    "axes[1, 0].set_title(\"Petal Length Distribution by Species\")\n",
    "axes[1, 0].set_xlabel(\"Petal Length (cm)\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].legend(title=\"Species\", labels=iris_df['class'].unique(), loc='upper right')\n",
    "\n",
    "# Plot histogram for Petal Width\n",
    "sns.histplot(data=iris_df, x=\"petal width\", hue=\"class\", kde=False, ax=axes[1, 1], bins=15)\n",
    "axes[1, 1].set_title(\"Petal Width Distribution by Species\")\n",
    "axes[1, 1].set_xlabel(\"Petal Width (cm)\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "axes[1, 1].legend(title=\"Species\", labels=iris_df['class'].unique(), loc='upper right')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "# Save the figure for histogram as a PNG file and show\n",
    "plt.savefig('histograms_by_species.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots look at the distribution of the data, this helps visualise overlap and separation of species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots - plot and save scatter plots for each pair of variables in the dataset as a png file.\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Scatter plot for sepal length vs width\n",
    "sns.scatterplot(ax=axes[0], data=iris_df, x='sepal length', y='sepal width', hue='class', s=100)\n",
    "axes[0].set_title('Sepal Length vs Sepal Width by Species')\n",
    "axes[0].set_xlabel('Sepal Length (cm)')\n",
    "axes[0].set_ylabel('Sepal Width (cm)')\n",
    "axes[0].legend(title=\"Species\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Scatter plot for petal length vs width\n",
    "sns.scatterplot(ax=axes[1], data=iris_df, x='petal length', y='petal width', hue='class', s=100)\n",
    "axes[1].set_title('Petal Length vs Petal Width by Species')\n",
    "axes[1].set_xlabel('Petal Length (cm)')\n",
    "axes[1].set_ylabel('Petal Width (cm)')\n",
    "axes[1].legend(title=\"Species\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scatterplot_by_species.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
