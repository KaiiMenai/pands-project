{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author: Kyra Menai Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I will give a further commentary on each part of the code I have written in analysis.py. Please refer to the analysis.py python file for the analysis code in a more cohesive piece. In this file it will be broken down and annotated as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Importing the modules and specific tools for the data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any analysis can be started, importing the correct tools is essential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the importing of module libraries the dataset needed to be sourced. This was sourced from the UC Irvine Machine Learning Repository and imported using the python import button. firt it was necessary to run the 'pip install ucimlrepo' in the terminal to install the ucimlrepo package. Following this the dataset was imported. It is important to know that the dataset was initially not imported as a DataFrame, rather as a file containing metadata and variables. the dataset was renamed 'iris' to make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset, fetch the dataset, define the data (as pandas dataframes), print metadata, and print the variable information to check that it worked.\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "iris = fetch_ucirepo(id=53) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with the analysis, saving the dataset as a .csv file for future reference was important. The dataset needed to be converted and for this to happen. First the x and y frames of the data were extracted, these were the features and targets, respectively. Then the metadata and variables were checked and changed to note form. Finally the features and targets were combined into a dataframe 'iris_df' and this was converted to a .csv using the '.to_csv' function. Upon successful completion \"Iris dataset has been successfully exported to a CSV!\" would be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - extracting x and y (as pandas dataframes) \n",
    "x = iris.data.features \n",
    "y = iris.data.targets \n",
    "\n",
    "# metadata - print was to check\n",
    "# print(iris.metadata) \n",
    "\n",
    "# variable information - print was to check\n",
    "# print(iris.variables) \n",
    "\n",
    "# Combine the features and targets into a single DataFrame (df) so it can be exported as a CSV\n",
    "iris_df = pd.concat([x, y], axis=1)\n",
    "\n",
    "# Exporting the DataFrame (df) to a CSV file\n",
    "iris_df.to_csv('D:/Data_Analytics/Modules/PandS/pands-project/iris.csv', index=False)\n",
    "print(\"Iris dataset has been successfully exported to a CSV!\") # Output - Iris dataset has been successfully exported to a CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to continuing with the data analysis, to ensure ease of data manipulation, the data for analysis was then inputted from the iris dataframe saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('D:/Data_Analytics/Modules/PandS/pands-project/iris.csv')\n",
    "\n",
    "print(iris_df) # This will print the dataframe into the terminal and also gi ve a brief summary of (150 rows x 5 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to directly save any text or plots directly to a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing output directly to a txt file: https://labex.io/tutorials/python-how-to-redirect-the-print-function-to-a-file-in-python-398057\n",
    "\n",
    "# FOR SAVING AS A TXT FILE AND APPENDING AS WE GO ON \n",
    "## First, create a file with some initial content\n",
    "#with open(\"append_example.txt\", \"w\") as file:\n",
    "#    print(\"\\nThis content is being added to the file.\", file=file)\n",
    "#    print(\"Appended on: X DATE\", file=file)\n",
    "    ## Now, append to the file\n",
    "#with open(\"append_example.txt\", \"a\") as file:\n",
    "#    print(\"\\nThis content is being appended to the file.\", file=file)\n",
    "#    print(\"Appended on: X DATE\", file=file)\n",
    "#print(\"Additional content has been appended to append_example.txt\")\n",
    "## Check the final content\n",
    "#print(\"\\nFinal content of the file:\")\n",
    "#with open(\"append_example.txt\", \"r\") as file:    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, basic data checks were conducted and written to a text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data checks - check for missing values, duplicates, and data types\n",
    "## Using the 'with' statement to handle file operations\n",
    "\n",
    "with open(\"basic_data_explore.txt\", \"w\") as file: # The (file=file) argument is important to remember as it makes sure Python knows to write to the file and not the terminal.\n",
    "    print(\"Basic data checks:\", file=file)\n",
    "    print(\"The shape of the dataset:\", file=file)\n",
    "    print(iris_df.shape, file=file)\n",
    "    print(\"The first 5 rows of the dataset:\", file=file)\n",
    "    print(iris_df.head(), file=file) # This will print the first 5 rows of the dataset.\n",
    "    print(\"The last 5 rows of the dataset:\", file=file)\n",
    "    print(iris_df.tail(), file=file) # This will print the last 5 rows of the dataset.\n",
    "    print(\"The column names of the dataset:\", file=file)\n",
    "    print(iris_df.columns, file=file) # This will print the column names of the dataset.\n",
    "    \n",
    "print(\"Basic data checks have been written to basic_data_explore.txt\")\n",
    "\n",
    "with open(\"basic_data_explore.txt\", \"a\") as file:\n",
    "    print(\"The number of rows and columns in the dataset:\", file=file)\n",
    "    print(iris_df.info(), file=file) # This will print the number of rows and columns in the dataset.\n",
    "    print(\"The number of missing values in the dataset:\", file=file)\n",
    "    print(iris_df.isnull().sum(), file=file) # This will print the number of missing values in the dataset.\n",
    "    print(\"The number of duplicate rows in the dataset:\", file=file)\n",
    "    print(iris_df.duplicated().sum(), file=file) # This will print the number of duplicate rows in the dataset.\n",
    "    print(\"The data types of each column in the dataset:\", file=file)\n",
    "    print(iris_df.dtypes, file=file) # This will print the data types of each column in the dataset.print(\"This is the initial content of the file.\", file=file)\n",
    "    \n",
    "print(\"Basic data checks have been appended to basic_data_explore.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting additional data analysis, it is important to remove any duplicate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make sure tha any duplicates are removed and that the data types are correct before conducting any analysis.\n",
    "# Already checked for missing values and we know there are 0, but there are 3 duplicate rows in the dataset.\n",
    "\n",
    "data = iris_df.drop_duplicates(subset=\"class\",) # This will remove any duplicate rows in the dataset, based on the class(species) column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
